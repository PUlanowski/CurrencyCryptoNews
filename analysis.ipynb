{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "import configparser\n",
    "import sql_queries\n",
    "import pandas as pd\n",
    "import pandas.io.sql as sqlio\n",
    "import logging as log\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Building spark session"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-cb3472585aae>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mfindspark\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\lib\\site-packages\\findspark.py\u001B[0m in \u001B[0;36minit\u001B[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001B[0m\n\u001B[0;32m    127\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    128\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mspark_home\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 129\u001B[1;33m         \u001B[0mspark_home\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    130\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    131\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mpython_path\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda3\\lib\\site-packages\\findspark.py\u001B[0m in \u001B[0;36mfind\u001B[1;34m()\u001B[0m\n\u001B[0;32m     34\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mspark_home\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     35\u001B[0m         raise ValueError(\n\u001B[1;32m---> 36\u001B[1;33m             \u001B[1;34m\"Couldn't find Spark, make sure SPARK_HOME env is set\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     37\u001B[0m             \u001B[1;34m\" or Spark is in an expected location (e.g. from homebrew installation).\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m         )\n",
      "\u001B[1;31mValueError\u001B[0m: Couldn't find Spark, make sure SPARK_HOME env is set or Spark is in an expected location (e.g. from homebrew installation)."
     ]
    }
   ],
   "source": [
    "findspark.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('postgres-cnn') \\\n",
    "        .master('spark://0.0.0.0:7077') \\\n",
    "        .config('spark.jars', 'C:\\\\Users\\\\pit\\\\Google Drive\\\\Udacity\\\\Capstone Project\\\\jdbc\\\\postgresql-42.2.5.jar',) \\\n",
    "        .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking if connection is successful"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .format('jdbc') \\\n",
    "    .option('url', 'jdbc:postgresql://localhost:5432/cnn') \\\n",
    "    .option('dbtable', 'ccy_crypto_stage') \\\n",
    "    .option('user', 'postgres') \\\n",
    "    .option('password', '') \\\n",
    "    .option('driver', 'org.postgresql.Driver') \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reading all tables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table fetched correctly\n"
     ]
    },
    {
     "data": {
      "text/plain": "    table_name\n3  dim_ccy_aed\n4  dim_ccy_aud\n5  dim_ccy_bhd\n6  dim_ccy_bnd\n7  dim_ccy_brl",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>table_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>dim_ccy_aed</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dim_ccy_aud</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>dim_ccy_bhd</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>dim_ccy_bnd</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>dim_ccy_brl</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('C:\\\\Users\\\\pit\\\\Google Drive\\\\Udacity\\\\Capstone Project\\\\cfg.cfg')\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=config['POSTGRES']['HOST'],\n",
    "    user=config['POSTGRES']['USER'],\n",
    "    database=config['POSTGRES']['DATABASE'],\n",
    "    password=config['POSTGRES']['PASS'],\n",
    "    port=config['POSTGRES']['PORT']\n",
    ")\n",
    "conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "cur = conn.cursor()\n",
    "\n",
    "table_names = sqlio.read_sql_query(sql_queries.get_table_names, conn)\n",
    "if table_names.head() is not None:\n",
    "    print('Table fetched correctly')\n",
    "else:\n",
    "    print('Table empty')\n",
    "    log.info('Table empty')\n",
    "\n",
    "tn = table_names[table_names.table_name.str.contains('dim*')]\n",
    "tn.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if we have all tables we can run psql query in our postgres\n",
    "```sql\n",
    "SELECT COUNT(table_name)\n",
    "FROM information_schema.tables\n",
    "WHERE table_name ~ '^dim*';\n",
    "```\n",
    "and compare to fetched results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All table names fetched\n"
     ]
    }
   ],
   "source": [
    "dim_tabs =2055\n",
    "if tn.count().values[0] == dim_tabs:\n",
    "    print('All table names fetched')\n",
    "else:\n",
    "    print('Tables missing!')\n",
    "    log.info('Tables missing!')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculating day-to-day delta of rates movement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark_dim = spark.read \\\n",
    "    .format('jdbc') \\\n",
    "    .option('url', 'jdbc:postgresql://localhost:5432/cnn') \\\n",
    "    .option('dbtable', 'dim_ccy_aed') \\\n",
    "    .option('user', 'postgres') \\\n",
    "    .option('password', '') \\\n",
    "    .option('driver', 'org.postgresql.Driver') \\\n",
    "    .load()\n",
    "\n",
    "spark_dim.printSchema()\n",
    "spark_dim.columns\n",
    "print(spark_dim.count())\n",
    "# spark_dim.select('closure_rate').describe().show()\n",
    "\n",
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-92ad23d4",
   "language": "python",
   "display_name": "PyCharm (Data_Modeling_with_Apache_Cassandra)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}